# -*- coding: utf-8 -*-
"""Social Media Sentiment & Topic Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n0-6be_uJ8lepeM_Z7-ctkdA6GowPb9Z
"""

!pip install pandas matplotlib seaborn nltk wordcloud gensim

!pip install --upgrade numpy

!pip install --upgrade pandas

!pip install --upgrade numpy
!pip install --upgrade pandas

!pip install --upgrade numpy pandas

!pip install --upgrade numpy
!pip install --upgrade gensim
# Reinstalling gensim after upgrading numpy ensures compatibility
# between the two libraries.

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from gensim import corpora
from gensim.models import LdaModel
import re
import warnings
warnings.filterwarnings("ignore")

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Load dataset
df = pd.read_csv('sentimentdataset.csv')

# Display first 5 rows
print(df.head(5))

# Data Cleaning
# Drop unnecessary columns
df.drop(columns=['Unnamed: 0', 'ID'], inplace=True)

# Convert Timestamp to datetime
df['Timestamp'] = pd.to_datetime(df['Timestamp'])

# Fill missing values (if any)
df['Likes'].fillna(0, inplace=True)
df['Retweets'].fillna(0, inplace=True)
df.dropna(subset=['Text', 'Sentiment', 'Platform'], inplace=True)

# Text Preprocessing
def preprocess_text(text):
    # Lowercase
    text = text.lower()
    # Remove URLs, mentions, hashtags, and special characters
    text = re.sub(r'http\S+|www\S+|@\w+|#\w+', '', text)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Tokenize
    tokens = word_tokenize(text)
    # Remove stopwords and lemmatize
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]
    return ' '.join(tokens)

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt_tab') # Download the punkt_tab resource

# Sentiment Analysis by Platform
plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='Platform', hue='Sentiment', palette='viridis')
plt.title('Sentiment Distribution by Platform')
plt.show()

# Hashtag Analysis
def extract_hashtags(text):
    return [tag.strip("#") for tag in text.split() if tag.startswith("#")]

df['Hashtags'] = df['Hashtags'].apply(lambda x: extract_hashtags(x) if isinstance(x, str) else [])

# Flatten hashtags list
all_hashtags = [hashtag for sublist in df['Hashtags'] for hashtag in sublist]

# Top 10 Hashtags
plt.figure(figsize=(12, 6))
pd.Series(all_hashtags).value_counts().head(10).plot(kind='bar', color='skyblue')
plt.title('Top 10 Hashtags')
plt.show()

# Topic Modeling (LDA)
# Prepare corpus
# Apply preprocess_text to the 'Text' column and create 'Clean_Text' column
df['Clean_Text'] = df['Text'].apply(preprocess_text)
texts = [text.split() for text in df['Clean_Text']]
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Train LDA model
lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=10)

# Print topics
print("Top Topics:")
for idx, topic in lda_model.print_topics(-1):
    print(f"Topic {idx}: {topic}")

# Time-Based Analysis
df.set_index('Timestamp', inplace=True)

# Resample data by month
monthly_posts = df.resample('M').size()

plt.figure(figsize=(12, 6))
monthly_posts.plot(title='Monthly Post Trends')
plt.show()

# Platform-Specific Insights
platform_analysis = df.groupby('Platform').agg({
    'Likes': 'mean',
    'Retweets': 'mean',
    'Sentiment': lambda x: (x == 'Positive').mean()
}).reset_index()

print("Platform Performance:")
print(platform_analysis)

# Word Cloud for Text
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(df['Clean_Text']))
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Common Terms')
plt.show()